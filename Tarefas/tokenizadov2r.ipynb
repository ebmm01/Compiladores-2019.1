{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "90bt5aGEC09y"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import io\n",
    "import tokenize\n",
    "\n",
    "\n",
    "BLACKLIST = {tokenize.NL, tokenize.INDENT, tokenize.DEDENT, \n",
    "             tokenize.NEWLINE, tokenize.ENDMARKER, tokenize.ENCODING}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0yPTLee5C099"
   },
   "outputs": [],
   "source": [
    "def make_lexer(defs, ignore=(), keywords=()):\n",
    "    \"\"\"\n",
    "    Retorna um lexer a partir de um dicionario com\n",
    "    definicoes de tokens\n",
    "    \"\"\"\n",
    "    defs['error'] = r'.+?'\n",
    "    named = [r'(?P<%s>%s)' % (k, v) \n",
    "             for (k, v) in defs.items()]\n",
    "    regex = re.compile('|'.join(named))\n",
    "    \n",
    "    def lexer(code):\n",
    "        tokens = []\n",
    "        line_no = 1\n",
    "        indent = 0\n",
    "        \n",
    "        for m in regex.finditer(code):\n",
    "            i, j = m.span()\n",
    "            data = m.string[i:j]\n",
    "            kind = m.lastgroup\n",
    "            \n",
    "            if kind == 'space':\n",
    "                line_no += data.count('\\n')\n",
    "            if data in keywords:\n",
    "                tokens.append(Token(data, 'keyword'))\n",
    "            elif kind == 'error':\n",
    "                raise ValueError(f'invalido: {data!r}')\n",
    "            elif kind not in ignore:\n",
    "                tokens.append((data, kind))\n",
    "            \n",
    "        return tokens\n",
    "        \n",
    "    return lexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0CRwU1QeC0-F"
   },
   "outputs": [],
   "source": [
    "py_lex = lambda code: list(py_tokens(code))\n",
    "\n",
    "def py_tokens(code):\n",
    "    fd = io.BytesIO(code.encode('utf8'))\n",
    "    tks = tokenize.tokenize(fd.__next__)\n",
    "    next(tks)\n",
    "    for tk in tks:\n",
    "        if tk.type not in BLACKLIST:\n",
    "            yield (tk.string, tokenize.tok_name[tk.type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rDNdhdHaC0-P"
   },
   "outputs": [],
   "source": [
    "# CRIE SEU LEXER AQUI\n",
    "my_lex = make_lexer({\n",
    "    'STRING': r'r?(\".+[\"]|\\'.+[\\'])|(\".+[^\"]|\\'.+[^\\'])',\n",
    "    #'STRING' : ''\n",
    "    'OP': r'\\+|\\-|\\*\\*?|==|!=|<=?|>=?|//?|\\[|\\]|\\{|\\}',\n",
    "    'NUMBER' : r'[+-]?(\\d+)(?:\\.(\\d*)(?:[eE]([+-])?(\\d+))?)?',\n",
    "    #'NUMBER': r'\\d+\\.?\\d*(?:[eE]([+-])?(\\d+))??:' + parte_fracionaria + exp + ')?',\n",
    "    'NAME': r'[a-zA-Z_]\\w*',\n",
    "    'SPACE': r'\\s+',\n",
    "    'ANY': r'.+?',\n",
    "}, ignore={'SPACE', 'COMMMENT'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "VHumBlA_C0-b",
    "outputId": "d55f7056-5604-4f5a-abd5-430f06050b92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "todos iguais? False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('@', 'ANY'), ('@', 'OP')),\n",
       " (('foobar', 'NAME'), ('foobar', 'NAME')),\n",
       " (('class', 'NAME'), ('class', 'NAME')),\n",
       " (('Foo', 'NAME'), ('Foo', 'NAME')),\n",
       " ((':', 'ANY'), (':', 'OP')),\n",
       " (('.', 'ANY'), ('...', 'OP'))]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code = \"\"\"\n",
    "@foobar\n",
    "class Foo:\n",
    "\t...\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "cmp = list(zip(my_lex(code), py_lex(code)))\n",
    "\n",
    "print('todos iguais?', all(x == y for x, y in cmp))\n",
    "\n",
    "cmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1hG0pDoVC0-9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "tokenizador.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
